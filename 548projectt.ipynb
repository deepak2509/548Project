{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74604f9f-658d-4087-be28-0da892ab08ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# There is currently no official GPU support for MacOS.\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12403c2-246c-4822-8aa6-136311c9364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax\n",
      "  Downloading flax-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jax>=0.4.27 (from flax)\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: msgpack in /opt/anaconda3/lib/python3.12/site-packages (from flax) (1.0.3)\n",
      "Collecting optax (from flax)\n",
      "  Downloading optax-0.2.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting orbax-checkpoint (from flax)\n",
      "  Downloading orbax_checkpoint-0.9.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorstore (from flax)\n",
      "  Downloading tensorstore-0.1.68-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1 in /opt/anaconda3/lib/python3.12/site-packages (from flax) (13.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/anaconda3/lib/python3.12/site-packages (from flax) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from flax) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from flax) (1.26.4)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.4.27->flax)\n",
      "  Downloading jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax>=0.4.27->flax)\n",
      "  Downloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax>=0.4.27->flax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from jax>=0.4.27->flax) (1.13.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=11.1->flax) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=11.1->flax) (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from optax->flax) (2.1.0)\n",
      "Collecting chex>=0.1.87 (from optax->flax)\n",
      "  Downloading chex-0.1.87-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting etils[epy] (from optax->flax)\n",
      "  Downloading etils-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/lib/python3.12/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from orbax-checkpoint->flax) (3.20.3)\n",
      "Collecting humanize (from orbax-checkpoint->flax)\n",
      "  Downloading humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax) (75.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax) (0.1.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.3.1)\n",
      "Collecting importlib_resources (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp in /opt/anaconda3/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
      "Downloading flax-0.10.1-py3-none-any.whl (419 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.3/419.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optax-0.2.4-py3-none-any.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orbax_checkpoint-0.9.1-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorstore-0.1.68-cp312-cp312-macosx_11_0_arm64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chex-0.1.87-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.4/99.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl (68.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl (750 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading humanize-4.11.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/128.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.10.0-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: ml-dtypes, importlib_resources, humanize, etils, tensorstore, jaxlib, jax, orbax-checkpoint, chex, optax, flax\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.1 requires ml-dtypes~=0.3.1, but you have ml-dtypes 0.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chex-0.1.87 etils-1.10.0 flax-0.10.1 humanize-4.11.0 importlib_resources-6.4.5 jax-0.4.35 jaxlib-0.4.35 ml-dtypes-0.5.0 optax-0.2.4 orbax-checkpoint-0.9.1 tensorstore-0.1.68\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39136aba-2eae-4a0f-9293-c33f4a35ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b498908-34e4-41e3-bb0a-02fb4693d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731780902.278620 5335202 service.cc:145] XLA service 0x600002327b00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731780902.279755 5335202 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1731780902.730273 5335202 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eat carrots.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"summarize: Studies show that eating carrots helps improve vision. Carrots contain beta-carotene, a substance that the body converts into vitamin A, crucial for maintaining healthy eyesight.\"\n",
    "\n",
    "# Generate inputs\n",
    "inputs = tokenizer(prompt, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "\n",
    "# Generate outputs\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43fcfa42-25a7-43c6-ba40-62165e318b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich freue mich sehr, die Schnee in diesem Winter zu sehen.\n"
     ]
    }
   ],
   "source": [
    "# Define the translation prompt\n",
    "translation_prompt = \"translate English to German: I'm very excited to see the snow this winter.\"\n",
    "\n",
    "# Prepare inputs and generate outputs\n",
    "translation_inputs = tokenizer(translation_prompt, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "translation_outputs = model.generate(translation_inputs[\"input_ids\"], max_length=40, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and display the translation\n",
    "print(tokenizer.decode(translation_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f926469-7a66-47be-9aaa-4ba3dc749337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# Define the context and question\n",
    "context_question = \"There are 29 states in India. How many states in India?\"\n",
    "\n",
    "# Generate inputs\n",
    "question_inputs = tokenizer(context_question, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "\n",
    "# Generate outputs\n",
    "question_outputs = model.generate(question_inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the answer\n",
    "print(tokenizer.decode(question_outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73b1102-c6a5-4b68-b5f8-b3e83e6de957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Organize your time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here, we provide a few examples along with their summaries to help the model understand the pattern.\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    \"Summarise: 'The quick brown fox jumps over the lazy dog. The dog was not amused by the fox's antics.' Summary: 'The fox jumped over the dog, who was not happy.'\",\n",
    "    \"Summarise: 'The rain in Spain stays mainly in the plain. It was a wet and rainy season in the Spanish plains.' Summary: 'It was a rainy season in the Spanish plains.'\"\n",
    "]\n",
    "\n",
    "# New prompt to summarize\n",
    "prompt = \"Summarise: 'Research indicates that time management is a key factor for success in graduate school. Master's students who prioritize tasks and maintain a structured schedule are better able to balance coursework, research, and personal responsibilities, leading to reduced stress and improved academic performance'\"\n",
    "\n",
    "# Combine examples and prompt\n",
    "combined_prompt = \"\\n\\n\".join(few_shot_examples + [prompt])\n",
    "\n",
    "# Generate inputs\n",
    "inputs = tokenizer(combined_prompt, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "\n",
    "# Generate outputs\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cac5024-9322-4e71-b0ea-1dcf4b02f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperando una vacación feliz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here, we provide a few examples along with their translations to help the model understand the pattern.\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    \"translate English to Spanish: 'The cat sits on the mat.' Translation: 'El gato se sienta en la alfombra.'\",\n",
    "    \"translate English to Spanish: 'The sun is shining brightly.' Translation: 'El sol brilla intensamente.'\"\n",
    "]\n",
    "\n",
    "# New prompt to translate\n",
    "translation_prompt = \"translate English to Spanish: 'wishing you a happy vacation'\"\n",
    "\n",
    "# Combine examples and prompt\n",
    "combined_translation_prompt = \"\\n\\n\".join(few_shot_examples + [translation_prompt])\n",
    "\n",
    "# Prepare inputs and generate outputs\n",
    "translation_inputs = tokenizer(combined_translation_prompt, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "translation_outputs = model.generate(translation_inputs[\"input_ids\"], max_length=40, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and display the translation\n",
    "print(tokenizer.decode(translation_outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04b42e0f-8282-44df-80b9-10f2ccb5fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Mt Everest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here, we provide a few examples along with their answers to help the model understand the pattern.\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    \"The Great Wall of China is over 13,000 miles long. question: 'How long is the Great Wall of China?' answer: 'The Great Wall of China is over 13,000 miles long.'\",\n",
    "    \"The capital of France is Paris. question: 'What is the capital of France?' answer: 'The capital of France is Paris.'\"\n",
    "]\n",
    "\n",
    "# New context and question\n",
    "context_question = \"question: 'What is the highest mountain in the world?'\"\n",
    "\n",
    "# Combine examples and prompt\n",
    "combined_qa_prompt = \"\\n\\n\".join(few_shot_examples + [context_question])\n",
    "\n",
    "# Generate inputs\n",
    "question_inputs = tokenizer(combined_qa_prompt, return_tensors=\"tf\", max_length=512, truncation=True, padding=True)\n",
    "\n",
    "# Generate outputs\n",
    "question_outputs = model.generate(question_inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the answer\n",
    "print(tokenizer.decode(question_outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57d970-0b13-433d-a677-56cfcdb70277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
